{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e45f6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import torch_optimizer as optim  # 提供 Ranger 优化器\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d1fb83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "batch_size = 128\n",
    "lr = 1e-3\n",
    "num_epochs = 20\n",
    "model_name = 'resnet50'  # 可选：'simplecnn' 或 'resnet50'\n",
    "\n",
    "# --- Setup MPS device ---\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2046c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        # 1×1 降维\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(planes)\n",
    "        # 3×3 空洞感受野\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(planes)\n",
    "        # 1×1 恢复维度\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3   = nn.BatchNorm2d(planes * self.expansion)\n",
    "        # 如果跨步或通道数变化，用下采样调整捷径\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "# def make_layer(in_planes, planes, blocks, stride=1):\n",
    "#     downsample = None\n",
    "#     out_planes = planes * Bottleneck.expansion\n",
    "#     if stride != 1 or in_planes != out_planes:\n",
    "#         # 用 1×1 卷积来匹配维度 & 跨步下采样\n",
    "#         downsample = nn.Sequential(\n",
    "#             nn.Conv2d(in_planes, out_planes,\n",
    "#                       kernel_size=1, stride=stride, bias=False),\n",
    "#             nn.BatchNorm2d(out_planes),\n",
    "#         )\n",
    "#     layers = [Bottleneck(in_planes, planes, stride, downsample)]\n",
    "#     for _ in range(1, blocks):\n",
    "#         layers.append(Bottleneck(out_planes, planes))\n",
    "#     return nn.Sequential(*layers)\n",
    "\n",
    "class SimpleResNetLike(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # 初始层（可以改成 3×3 conv + BN + ReLU，去掉大核7×7）\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        # 四个 stage，block 数量可按 ResNet-50 ([3,4,6,3]) 或简化\n",
    "        self.layer1 = self.make_layer( 64,  64, blocks=3, stride=1)  # 输出 256 通道\n",
    "        self.layer2 = self.make_layer(256, 128, blocks=4, stride=2)  # 输出 512 通道\n",
    "        self.layer3 = self.make_layer(512, 256, blocks=6, stride=2)  # 输出1024 通道\n",
    "        self.layer4 = self.make_layer(1024,512, blocks=3, stride=2)  # 输出2048 通道\n",
    "\n",
    "        # 全局池化 + 全连接\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512 * Bottleneck.expansion, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def make_layer(self, in_planes, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        out_planes = planes * Bottleneck.expansion\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            # 用 1×1 卷积来匹配维度 & 跨步下采样\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes,\n",
    "                        kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_planes),\n",
    "            )\n",
    "        layers = [Bottleneck(in_planes, planes, stride, downsample)]\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(out_planes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # He 初始化\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                # 全连接层也可以做类似初始化\n",
    "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='linear')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                # BN 的 weight 初始化为 1，bias 为 0\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6fd21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可选模型：SimpleCNN 或 ResNet50\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 32 * 32, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def get_model(name='resnet50', num_classes=10):\n",
    "    if name.lower() == 'resnet50':\n",
    "        model = torchvision.models.resnet50(pretrained=False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    else:\n",
    "        model = SimpleResNetLike(num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42394a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, device, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, device, loader, criterion):\n",
    "    model.eval()\n",
    "    correct, total_loss = 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(targets).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a0ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_index(full_train, train_ratio=0.8):\n",
    "    train_indices, val_indices = random_split(\n",
    "        list(range(len(full_train))),\n",
    "        [int(len(full_train) * train_ratio), len(full_train) - int(len(full_train) * train_ratio)]\n",
    "    )\n",
    "    return train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7a4969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_augmentation(train_indices, val_indices):\n",
    "    # 数据增强与标准化\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "\n",
    "        transforms.RandomCrop(128, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2430, 0.2610))\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "\n",
    "        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2430, 0.2610))\n",
    "    ])\n",
    "\n",
    "    train_set = Subset(\n",
    "        torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform_train),\n",
    "        train_indices.indices if hasattr(train_indices, 'indices') else train_indices\n",
    "    )\n",
    "    val_set = Subset(\n",
    "        torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform_test),\n",
    "        val_indices.indices if hasattr(val_indices, 'indices') else val_indices\n",
    "    )\n",
    "\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "872ef019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 加载训练集并拆分为 train/val\n",
    "full_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "train_indices, val_indices = split_train_val_index(full_train, train_ratio=0.8)\n",
    "\n",
    "# get train/val/test sets after data pre-processing and augmentation\n",
    "train_set, val_set, test_set = get_data_augmentation(train_indices, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1de3134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger (\n",
      "Parameter Group 0\n",
      "    N_sma_threshhold: 5\n",
      "    alpha: 0.5\n",
      "    betas: (0.95, 0.999)\n",
      "    eps: 1e-05\n",
      "    k: 6\n",
      "    lr: 0.001\n",
      "    step_counter: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 00 | Train Loss: 1.9357 | Val Loss: 1.6175 | Val Acc: 41.58%\n",
      "Epoch 01 | Train Loss: 1.5166 | Val Loss: 1.4149 | Val Acc: 48.95%\n",
      "Epoch 02 | Train Loss: 1.2782 | Val Loss: 1.3064 | Val Acc: 53.91%\n",
      "Epoch 03 | Train Loss: 1.0907 | Val Loss: 1.0505 | Val Acc: 62.52%\n",
      "Epoch 04 | Train Loss: 0.9225 | Val Loss: 1.0745 | Val Acc: 62.75%\n",
      "Epoch 05 | Train Loss: 0.8163 | Val Loss: 0.8176 | Val Acc: 71.36%\n",
      "Epoch 06 | Train Loss: 0.7186 | Val Loss: 0.7963 | Val Acc: 72.60%\n",
      "Epoch 07 | Train Loss: 0.6406 | Val Loss: 0.7573 | Val Acc: 74.21%\n",
      "Epoch 08 | Train Loss: 0.5677 | Val Loss: 1.0246 | Val Acc: 67.82%\n",
      "Epoch 09 | Train Loss: 0.5127 | Val Loss: 0.7489 | Val Acc: 75.75%\n",
      "Epoch 10 | Train Loss: 0.4673 | Val Loss: 0.8663 | Val Acc: 72.51%\n",
      "Epoch 11 | Train Loss: 0.4207 | Val Loss: 0.5818 | Val Acc: 80.23%\n",
      "Epoch 12 | Train Loss: 0.3809 | Val Loss: 0.5607 | Val Acc: 81.44%\n",
      "Epoch 13 | Train Loss: 0.3522 | Val Loss: 0.5828 | Val Acc: 81.13%\n",
      "Epoch 14 | Train Loss: 0.3203 | Val Loss: 0.7049 | Val Acc: 78.49%\n",
      "Epoch 15 | Train Loss: 0.3080 | Val Loss: 0.7530 | Val Acc: 78.13%\n",
      "Epoch 16 | Train Loss: 0.2730 | Val Loss: 0.7562 | Val Acc: 77.99%\n",
      "Epoch 17 | Train Loss: 0.2596 | Val Loss: 0.5357 | Val Acc: 83.38%\n",
      "Epoch 18 | Train Loss: 0.2347 | Val Loss: 0.5865 | Val Acc: 82.57%\n",
      "Epoch 19 | Train Loss: 0.2214 | Val Loss: 0.5919 | Val Acc: 82.15%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 模型、损失、优化器\n",
    "model = get_model(\"SimpleResNetLike\").to(device)\n",
    "\n",
    "# Use ResNet50 as the model\n",
    "num_classes = 10\n",
    "# model = torchvision.models.resnet50(pretrained=False)\n",
    "# model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# model = SimpleCNN(num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Ranger(model.parameters(), lr=lr)\n",
    "scaler = GradScaler()  # 用于混合精度训练\n",
    "\n",
    "# get the information of the optimizer\n",
    "print(optimizer)\n",
    "\n",
    "# 训练与验证\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, device, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, device, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc * 100:.2f}%\")\n",
    "    # 保存最优模型\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), './model/best_model_cnn_cifar.pth')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c410a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y0/5zx_gdxj5rvb7c3v91ml3v780000gn/T/ipykernel_4176/1345411143.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./model/best_model_cnn_cifar.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Loss: 0.5429 | Final Test Acc: 83.48%\n"
     ]
    }
   ],
   "source": [
    "# 测试集评估\n",
    "model.load_state_dict(torch.load('./model/best_model_cnn_cifar.pth'))\n",
    "test_loss, test_acc = evaluate(model, device, test_loader, criterion)\n",
    "print(f\"\\nFinal Test Loss: {test_loss:.4f} | Final Test Acc: {test_acc * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
